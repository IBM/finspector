{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d38608-2ed6-4d17-9ce2-b35ad826a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finspector\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AlbertTokenizer, AlbertForMaskedLM, RobertaTokenizer, RobertaForMaskedLM, DistilBertTokenizer, DistilBertModel, MPNetTokenizer, MPNetModel, PegasusForConditionalGeneration, PegasusTokenizer, T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b363b1-5427-4870-9ad4-fb73045e88c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Prepare Three Large Language Models, BERT, RoBERTa, and ALBERT to Generate Pseudo-Log-Likelihoods with them.\n",
    "\n",
    "model_names = ['bert', 'roberta', 'albert'] ## , 'distilbert', 'mpnet' generates errors\n",
    "models_to_test = []\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    if model_name == \"bert\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "        uncased = True\n",
    "    elif model_name == \"roberta\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "        model = RobertaForMaskedLM.from_pretrained('roberta-large')\n",
    "        uncased = False\n",
    "    elif model_name == \"albert\":\n",
    "        tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2')\n",
    "        model = AlbertForMaskedLM.from_pretrained('albert-xxlarge-v2')\n",
    "        uncased = False\n",
    "    elif model_name == 'distilbert':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        uncased = True\n",
    "    elif model_name =='mpnet':\n",
    "        tokenizer = MPNetTokenizer.from_pretrained(\"microsoft/mpnet-base\")\n",
    "        model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\")\n",
    "        uncased = False\n",
    "    \n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "\n",
    "    lm = {\n",
    "        \"name\": model_name,\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"mask_token\": mask_token,\n",
    "        \"log_softmax\": log_softmax,\n",
    "        \"uncased\": uncased\n",
    "    }\n",
    "    \n",
    "    models_to_test.append(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068e3b14-42b4-437a-bb3c-d6f6123cca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores functions.\n",
    "# Original Code: https://github.com/nyu-mll/crows-pairs\n",
    "\n",
    "def get_log_prob_unigram(masked_token_ids, token_ids, mask_idx, lm):\n",
    "    \"\"\"\n",
    "    Given a sequence of token ids, with one masked token, return the log probability of the masked token.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = lm[\"model\"]\n",
    "    tokenizer = lm[\"tokenizer\"]\n",
    "    log_softmax = lm[\"log_softmax\"]\n",
    "    mask_token = lm[\"mask_token\"]\n",
    "    uncased = lm[\"uncased\"]\n",
    "\n",
    "    # get model hidden states\n",
    "    output = model(masked_token_ids)\n",
    "    hidden_states = output[0].squeeze(0)\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "\n",
    "    # we only need log_prob for the MASK tokens\n",
    "    assert masked_token_ids[0][mask_idx] == mask_id\n",
    "\n",
    "    hs = hidden_states[mask_idx]\n",
    "    target_id = token_ids[0][mask_idx]\n",
    "    log_probs = log_softmax(hs)[target_id]\n",
    "\n",
    "    return log_probs\n",
    "\n",
    "def get_span(seq):\n",
    "    seq = [str(x) for x in seq.tolist()]\n",
    "    template = [x for x in range(len(seq))]\n",
    "    return template\n",
    "\n",
    "\n",
    "def get_score(sent, lm, n=1):\n",
    "    \"\"\"\n",
    "    Score each sentence by masking one word at a time.\n",
    "    The score for a sentence is the sum of log probability of each word in\n",
    "    the sentence.\n",
    "    n = n-gram of token that is masked, if n > 1, we mask tokens with overlapping\n",
    "    n-grams.\n",
    "    \"\"\"\n",
    "    model = lm[\"model\"]\n",
    "    tokenizer = lm[\"tokenizer\"]\n",
    "    log_softmax = lm[\"log_softmax\"]\n",
    "    mask_token = lm[\"mask_token\"]\n",
    "    uncased = lm[\"uncased\"]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    if uncased:\n",
    "        sent = sent.lower()\n",
    "\n",
    "    # tokenize\n",
    "    sent_token_ids = tokenizer.encode(sent, return_tensors='pt')\n",
    "\n",
    "    # get spans of non-changing tokens\n",
    "    template = get_span(sent_token_ids[0])\n",
    "\n",
    "    N = len(template)  # num. of tokens that can be masked\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "    \n",
    "    sent_log_probs = 0.\n",
    "    total_masked_tokens = 0\n",
    "\n",
    "    # skipping CLS and SEP tokens, they'll never be masked\n",
    "    for i in range(1, N-1):\n",
    "        sent_masked_token_ids = sent_token_ids.clone().detach()\n",
    "\n",
    "        sent_masked_token_ids[0][template[i]] = mask_id\n",
    "        total_masked_tokens += 1\n",
    "\n",
    "        score = get_log_prob_unigram(sent_masked_token_ids, sent_token_ids, template[i], lm)\n",
    "\n",
    "        sent_log_probs += score.item()\n",
    "\n",
    "    lpscore = {}\n",
    "    # average over iterations\n",
    "    lpscore[\"model\"] = lm['name']\n",
    "    lpscore[\"sentence\"] = sent\n",
    "    lpscore[\"score\"] = sent_log_probs/(N-2)\n",
    "\n",
    "    return lpscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daad8407-1889-4bf8-83ae-f1636df268bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentence embeddings using t-SNE\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "df = pd.read_csv('data/sample-movie-quotes.csv', index_col=False)\n",
    "df['index'] = [item for item in range(df.shape[0])]\n",
    "embvec = TSNE(n_components=2, perplexity=2).fit_transform(df[['bert', 'roberta', 'albert']]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b4bf83-fb1c-42ba-80be-ba0afec532fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86aa139c5b04e8c868f5ec18279c3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Visualize(data=[{'bias_type': 'godfather', 'sent_index': 0, 'para_index': 0, 'stereo_type': 0, 'more_or_less':â€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launch Finspector\n",
    "lh = finspector.Visualize(df, embedding = embvec, models = models_to_test, score_fn = get_score, sent_col='sent', cat_col='bias_type', spl_col='stereo_type', \n",
    "                          model_cols=['bert', 'roberta', 'albert'], other_cols=['sent_index', 'stereo_type'])\n",
    "lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13895775-c63e-4efb-8c6a-96d3226d1c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
